{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from fgvclib.apis import *\n",
    "from fgvclib.configs import FGVCConfig\n",
    "from fgvclib.utils.visualization import VOXEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASETS:\n",
      "  ROOT: /data/wangxinran/dataset/Birds2\n",
      "  TEST:\n",
      "    BATCH_SIZE: 16\n",
      "    NUM_WORKERS: 4\n",
      "    PIN_MEMORY: False\n",
      "    POSITIVE: None\n",
      "    SHUFFLE: False\n",
      "  TRAIN:\n",
      "    BATCH_SIZE: 16\n",
      "    NUM_WORKERS: 4\n",
      "    PIN_MEMORY: True\n",
      "    POSITIVE: None\n",
      "    SHUFFLE: True\n",
      "EPOCH_NUM: 1\n",
      "EXP_NAME: Baseline_ResNet50\n",
      "FIFTYONE:\n",
      "  NAME: BirdsTest\n",
      "  STORE: True\n",
      "INTERPRETER:\n",
      "  METHOD: gradcam\n",
      "  NAME: cam\n",
      "  TARGET_LAYERS: ['layer4']\n",
      "ITERATION_NUM: None\n",
      "LOGGER:\n",
      "  FILE_PATH: ./logs/\n",
      "  NAME: txt_logger\n",
      "  PRINT_FRE: 50\n",
      "METRICS: [{'name': 'accuracy(topk=1)', 'metric': 'accuracy', 'top_k': 1, 'threshold': None}, {'name': 'recall(threshold=0.5)', 'metric': 'recall', 'top_k': None, 'threshold': 0.5}, {'name': 'precision(threshold=0.5)', 'metric': 'precision', 'top_k': None, 'threshold': 0.5}]\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    ARGS: [{'pretrained': True}, {'del_keys': []}]\n",
      "    NAME: resnet50\n",
      "  CLASS_NUM: 200\n",
      "  CRITERIONS: [{'name': 'cross_entropy_loss', 'args': [], 'w': 1.0}]\n",
      "  ENCODING:\n",
      "    ARGS: None\n",
      "    NAME: GlobalAvgPooling\n",
      "  HEADS:\n",
      "    ARGS: [{'in_dim': [2048]}]\n",
      "    NAME: classifier_1fc\n",
      "  NAME: Baseline_ResNet50\n",
      "  NECKS:\n",
      "    ARGS: None\n",
      "    NAME: None\n",
      "  OUTPUTS_NUM: None\n",
      "OPTIMIZER:\n",
      "  LR:\n",
      "    backbone: 0.0002\n",
      "    encoding: 0.002\n",
      "    heads: 0.002\n",
      "    necks: 0.002\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: SGD\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "PER_EPOCH: None\n",
      "PER_ITERATION: None\n",
      "PROJ_NAME: FGVC\n",
      "RESUME_WEIGHT: None\n",
      "START_EPOCH: 0\n",
      "TRANSFORMS:\n",
      "  TEST: [{'name': 'resize', 'size': [600, 600]}, {'name': 'center_crop', 'size': 448}, {'name': 'to_tensor'}, {'name': 'normalize', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5]}]\n",
      "  TRAIN: [{'name': 'resize', 'size': [600, 600]}, {'name': 'random_crop', 'size': 448, 'padding': 8}, {'name': 'random_horizontal_flip', 'prob': 0.5}, {'name': 'to_tensor'}, {'name': 'normalize', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5]}]\n",
      "UPDATE_STRATEGY: general_updating\n",
      "USE_CUDA: True\n",
      "WEIGHT:\n",
      "  NAME: baseline_resnet50.pth\n",
      "  SAVE_DIR: /data/wangxinran/model/fgvclib/\n"
     ]
    }
   ],
   "source": [
    "config_file = \"./configs/baseline_restnet50.yml\"\n",
    "config = FGVCConfig()\n",
    "config.load(config_file)\n",
    "cfg = config.cfg\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5794/5794 [00:01<00:00, 3292.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5794/5794 [5.2s elapsed, 0s remaining, 1.2K samples/s]       \n",
      "  10% |█------------------|  1/10 [1.0m elapsed, 9.0m remaining, 0.0 samples/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3889/704522883.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_interpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvoxel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOXEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFIFTYONE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mvoxel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvoxel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/The-Greatest-Work/fgvclib/utils/visualization/voxel.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, transforms, n, name, seed, explain)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"heatmap\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHeatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/The-Greatest-Work/fgvclib/utils/interpreter/cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image_path, image_tensor, transforms, targets, save)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug_smooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigen_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigen_smooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrayscale_cam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maug_smooth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             return self.forward_augmentation_smoothing(\n\u001b[0;32m--> 186\u001b[0;31m                 input_tensor, targets, eigen_smooth)\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         return self.forward(input_tensor,\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward_augmentation_smoothing\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    160\u001b[0m             cam = self.forward(augmented_tensor,\n\u001b[1;32m    161\u001b[0m                                \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                eigen_smooth)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# The ttach library expects a tensor of size BxCxHxW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     95\u001b[0m         cam_per_layer = self.compute_cam_per_layer(input_tensor,\n\u001b[1;32m     96\u001b[0m                                                    \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                                                    eigen_smooth)\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_multi_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_per_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mcompute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    130\u001b[0m                                      \u001b[0mlayer_activations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                      \u001b[0mlayer_grads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                      eigen_smooth)\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_cam_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mget_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mweighted_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigen_smooth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_2d_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/pytorch_grad_cam/utils/svd_on_activations.py\u001b[0m in \u001b[0;36mget_2d_projection\u001b[0;34m(activation_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreshaped_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshaped_activations\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mreshaped_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshaped_activations\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mVT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/DevelopEnvironment/anaconda3/envs/DomainAug/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.MODEL)\n",
    "weight_path = os.path.join(cfg.WEIGHT.SAVE_DIR, cfg.WEIGHT.NAME)\n",
    "assert os.path.exists(weight_path), f\"The resume weight {cfg.RESUME_WEIGHT} dosn't exists.\"\n",
    "state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "if cfg.USE_CUDA:\n",
    "    assert torch.cuda.is_available(), f\"Cuda is not available.\"\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "transforms = build_transforms(cfg.TRANSFORMS.TEST)\n",
    "loader = build_dataset(root=os.path.join(cfg.DATASETS.ROOT, 'test'), cfg=cfg.DATASETS.TEST, transforms=transforms)\n",
    "\n",
    "interpreter = build_interpreter(model, cfg)\n",
    "voxel = VOXEL(dataset=loader.dataset, name=cfg.FIFTYONE.NAME, interpreter=interpreter)\n",
    "voxel.predict(model, transforms, 10, cfg.MODEL.NAME)\n",
    "voxel.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DomainAug')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd6b06855f274f2277a2f7686fea68f916535735cd03bfd17d6cd4ef91dd7383"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
